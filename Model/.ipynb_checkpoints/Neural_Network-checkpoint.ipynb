{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../features/features_global.csv\", sep=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blue      7037\n",
       "purple    6696\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[:,0].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_nulls = df.apply(lambda row : \n",
    "          any([ e == (\"null\") for e in row ])\n",
    "       , axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_nulls[212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>purple</td>\n",
       "      <td>3032929911</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>394.617</td>\n",
       "      <td>1.92308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>2.78571</td>\n",
       "      <td>158539</td>\n",
       "      <td>21724.8</td>\n",
       "      <td>24917.4</td>\n",
       "      <td>...</td>\n",
       "      <td>30202.8</td>\n",
       "      <td>531.632</td>\n",
       "      <td>5.44737</td>\n",
       "      <td>5.03158</td>\n",
       "      <td>0.363158</td>\n",
       "      <td>-0.242105</td>\n",
       "      <td>260.316</td>\n",
       "      <td>413.189</td>\n",
       "      <td>-8.27895</td>\n",
       "      <td>1.35263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1         2        3        4         5        6    \\\n",
       "212  purple  3032929911  0.428571  394.617  1.92308  0.923077  2.78571   \n",
       "\n",
       "        7        8        9     ...         162      163      164      165  \\\n",
       "212  158539  21724.8  24917.4   ...     30202.8  531.632  5.44737  5.03158   \n",
       "\n",
       "          166       167      168      169      170      171  \n",
       "212  0.363158 -0.242105  260.316  413.189 -8.27895  1.35263  \n",
       "\n",
       "[1 rows x 172 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[[212]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13733,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_nulls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[~no_nulls]\n",
    "df = df.apply(pd.to_numeric, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blue</td>\n",
       "      <td>2984814498</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>423.95993</td>\n",
       "      <td>1.917431</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>133402.100</td>\n",
       "      <td>17926.000</td>\n",
       "      <td>16480.800</td>\n",
       "      <td>...</td>\n",
       "      <td>13711.350</td>\n",
       "      <td>114.10000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>1.685000</td>\n",
       "      <td>-0.402500</td>\n",
       "      <td>-0.352500</td>\n",
       "      <td>193.48500</td>\n",
       "      <td>332.70502</td>\n",
       "      <td>-10.030000</td>\n",
       "      <td>-7.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>purple</td>\n",
       "      <td>3034035764</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>402.25280</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.157895</td>\n",
       "      <td>137797.310</td>\n",
       "      <td>17697.053</td>\n",
       "      <td>17959.790</td>\n",
       "      <td>...</td>\n",
       "      <td>17441.264</td>\n",
       "      <td>302.21054</td>\n",
       "      <td>1.189474</td>\n",
       "      <td>1.642105</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.081579</td>\n",
       "      <td>200.77895</td>\n",
       "      <td>350.03687</td>\n",
       "      <td>4.418421</td>\n",
       "      <td>30.384210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>purple</td>\n",
       "      <td>3036731710</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>421.79210</td>\n",
       "      <td>2.215569</td>\n",
       "      <td>1.215569</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>154794.270</td>\n",
       "      <td>23505.592</td>\n",
       "      <td>24949.682</td>\n",
       "      <td>...</td>\n",
       "      <td>29733.684</td>\n",
       "      <td>782.63160</td>\n",
       "      <td>0.326316</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>-0.036842</td>\n",
       "      <td>-0.413158</td>\n",
       "      <td>223.42105</td>\n",
       "      <td>339.43683</td>\n",
       "      <td>-9.331579</td>\n",
       "      <td>-31.594736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>purple</td>\n",
       "      <td>3018436026</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>384.25073</td>\n",
       "      <td>1.917526</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>2.368421</td>\n",
       "      <td>136257.270</td>\n",
       "      <td>17298.736</td>\n",
       "      <td>18882.053</td>\n",
       "      <td>...</td>\n",
       "      <td>14255.750</td>\n",
       "      <td>136.25000</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>-0.330000</td>\n",
       "      <td>-0.022500</td>\n",
       "      <td>165.56500</td>\n",
       "      <td>310.27000</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>65.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blue</td>\n",
       "      <td>3026930091</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>398.31564</td>\n",
       "      <td>2.887755</td>\n",
       "      <td>1.887755</td>\n",
       "      <td>4.173913</td>\n",
       "      <td>125274.305</td>\n",
       "      <td>17986.130</td>\n",
       "      <td>22280.957</td>\n",
       "      <td>...</td>\n",
       "      <td>16022.895</td>\n",
       "      <td>226.68420</td>\n",
       "      <td>7.163158</td>\n",
       "      <td>6.894737</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.323684</td>\n",
       "      <td>288.94210</td>\n",
       "      <td>461.35263</td>\n",
       "      <td>24.378946</td>\n",
       "      <td>56.265793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0           1         2          3         4         5         6    \\\n",
       "0    blue  2984814498  0.650000  423.95993  1.917431  0.917431  2.900000   \n",
       "1  purple  3034035764  0.526316  402.25280  2.000000  1.000000  2.157895   \n",
       "2  purple  3036731710  0.409091  421.79210  2.215569  1.215569  3.636364   \n",
       "3  purple  3018436026  0.473684  384.25073  1.917526  0.917526  2.368421   \n",
       "4    blue  3026930091  0.478261  398.31564  2.887755  1.887755  4.173913   \n",
       "\n",
       "          7          8          9      ...            162        163  \\\n",
       "0  133402.100  17926.000  16480.800    ...      13711.350  114.10000   \n",
       "1  137797.310  17697.053  17959.790    ...      17441.264  302.21054   \n",
       "2  154794.270  23505.592  24949.682    ...      29733.684  782.63160   \n",
       "3  136257.270  17298.736  18882.053    ...      14255.750  136.25000   \n",
       "4  125274.305  17986.130  22280.957    ...      16022.895  226.68420   \n",
       "\n",
       "        164       165       166       167        168        169        170  \\\n",
       "0  1.190000  1.685000 -0.402500 -0.352500  193.48500  332.70502 -10.030000   \n",
       "1  1.189474  1.642105  0.100000 -0.081579  200.77895  350.03687   4.418421   \n",
       "2  0.326316  0.773684 -0.036842 -0.413158  223.42105  339.43683  -9.331579   \n",
       "3  0.235000  0.405000 -0.330000 -0.022500  165.56500  310.27000   0.042500   \n",
       "4  7.163158  6.894737  0.857895  0.323684  288.94210  461.35263  24.378946   \n",
       "\n",
       "         171  \n",
       "0  -7.407500  \n",
       "1  30.384210  \n",
       "2 -31.594736  \n",
       "3  65.232500  \n",
       "4  56.265793  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13660, 172)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = df.replace(to_replace='null', value=0)\n",
    "#df = df.replace(to_replace='infinity', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Global constants\n",
    "seed = 7875\n",
    "validation_size = 750\n",
    "feature_count = df.shape[1] - 2\n",
    "\n",
    "#feed forward neural net\n",
    "n_nodes_hl1 = 300\n",
    "n_nodes_hl2 = 100\n",
    "n_nodes_hl3 = 50\n",
    "\n",
    "#cycles of feed forward + backprop on all K-folded samples\n",
    "hm_epochs = 200\n",
    "\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X = np.asarray(df.ix[:,2:feature_count+2])\n",
    "#standardize X\n",
    "meanX = np.mean(X, axis = 0)\n",
    "stdX = np.std(X, axis = 0)\n",
    "\n",
    "f = open('mean.pckl', 'wb')\n",
    "pickle.dump(meanX, f)\n",
    "f.close()\n",
    "\n",
    "f = open('std.pckl', 'wb')\n",
    "pickle.dump(stdX, f)\n",
    "f.close()\n",
    "\n",
    "X = (X - meanX) / stdX\n",
    "\n",
    "Y_1 = np.asarray(df.ix[:,0])\n",
    "Y_1 = [int(y == \"purple\") for y in Y_1]\n",
    "#one hot Y\n",
    "Y = np.zeros(shape=(len(Y_1), n_classes))\n",
    "Y[np.arange(len(Y_1)), Y_1] = 1\n",
    "    \n",
    "validation_features = X[:validation_size]\n",
    "validation_labels = Y[:validation_size]\n",
    "\n",
    "train_features = X[validation_size:]\n",
    "train_labels = Y[validation_size:]\n",
    "\n",
    "num_examples = train_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    \n",
    "    dropout_prob = 0.5\n",
    "    \n",
    "    hidden_1_layer = {\n",
    "        'weights': tf.Variable(tf.truncated_normal([feature_count, n_nodes_hl1], stddev=0.1, seed=seed)),\n",
    "        'biases': tf.Variable(tf.constant(1.0, shape=[n_nodes_hl1]))\n",
    "    }\n",
    "    \n",
    "    hidden_2_layer = {\n",
    "        'weights': tf.Variable(tf.truncated_normal([n_nodes_hl1, n_nodes_hl2], stddev=0.1, seed=seed)),\n",
    "        'biases': tf.Variable(tf.constant(1.0, shape=[n_nodes_hl2]))\n",
    "    }\n",
    "    \n",
    "    hidden_3_layer = {\n",
    "        'weights': tf.Variable(tf.truncated_normal([n_nodes_hl2, n_nodes_hl3], stddev=0.1, seed=seed)),\n",
    "        'biases': tf.Variable(tf.constant(1.0, shape=[n_nodes_hl3]))\n",
    "    }\n",
    "    \n",
    "    output_layer = {\n",
    "        'weights': tf.Variable(tf.truncated_normal([n_nodes_hl1, n_classes], stddev=0.1, seed=seed)),\n",
    "        'biases': tf.Variable(tf.constant(1.0, shape=[n_classes]))\n",
    "    }\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu6(l1)\n",
    "    \n",
    "    l1_drop = tf.nn.dropout(l1, dropout_prob, seed=seed)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1_drop, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu6(l2)\n",
    "    \n",
    "    l2_drop = tf.nn.dropout(l2, dropout_prob, seed=seed)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.sigmoid(l3)\n",
    "    \n",
    "    l3_drop = tf.nn.dropout(l3, dropout_prob, seed=seed)\n",
    "    \n",
    "    output = tf.matmul(l1_drop, output_layer['weights']) +  output_layer['biases']\n",
    "    \n",
    "    regularizers = (tf.nn.l2_loss(hidden_1_layer['weights']) + tf.nn.l2_loss(hidden_1_layer['biases']) +\n",
    "                                tf.nn.l2_loss(output_layer['weights']) + tf.nn.l2_loss(output_layer['biases']))\n",
    "    \n",
    "    return output, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_stat(x_range, trains, tests, vals, acc_0s, acc_1s):\n",
    "    \n",
    "    #plt.plot(x_range, trains,'-b', label='Training acc')\n",
    "    #plt.plot(x_range, vals,'-g', label='Validation acc')\n",
    "    #plt.plot(x_range, tests,'-y', label='Test acc')\n",
    "    plt.plot(x_range, acc_0s,'-r', label='Acc Class 0')\n",
    "    plt.plot(x_range, acc_1s,'-k', label='Acc Class 1')\n",
    "\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.0)\n",
    "\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_network_CV(x, lambda_):\n",
    "    \n",
    "    vals = []\n",
    "    trains = []\n",
    "    tests = []\n",
    "    x_range = []\n",
    "    \n",
    "    f1_vals = []\n",
    "    \n",
    "    acc_1s = []\n",
    "    acc_0s = []\n",
    "    \n",
    "    prediction, regularizers = neural_network_model(x)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y))\n",
    "    #log_loss = tf.contrib.losses.log_loss(predictions=prediction, labels=y)\n",
    "\n",
    "    #Eval this to get probability of [winning,losing]\n",
    "    prob = tf.nn.softmax(prediction)\n",
    "    \n",
    "    #learning rate can be passed\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost + lambda_ * regularizers)\n",
    "    \n",
    "    #metrics\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "    false_prediction = tf.logical_not(correct_prediction)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "\n",
    "    #use for f1 score if needed\n",
    "    true_positives = tf.reduce_sum(tf.to_int32(tf.logical_and(correct_prediction, tf.equal(tf.argmax(tf.nn.softmax(y),1), True) )))\n",
    "    false_positives = tf.reduce_sum(tf.to_int32(tf.logical_and(false_prediction, tf.equal(tf.argmax(tf.nn.softmax(y),1), True) )))\n",
    "    true_negatives = tf.reduce_sum(tf.to_int32(tf.logical_and(correct_prediction, tf.equal(tf.argmax(tf.nn.softmax(y),1), False) )))\n",
    "    false_negatives = tf.reduce_sum(tf.to_int32(tf.logical_and(false_prediction, tf.equal(tf.argmax(tf.nn.softmax(y),1), False) )))\n",
    "\n",
    "    #acc for each class\n",
    "    class_0 = tf.where(tf.equal(tf.argmax(y, 1), 0))\n",
    "    class_0 = tf.reshape(class_0, [tf.shape(class_0)[0]])\n",
    "    pred_0 = tf.gather(prediction, class_0)\n",
    "    y_0 = tf.gather(y, class_0)\n",
    "    class_0_correct = tf.equal(tf.argmax(pred_0,1), tf.argmax(y_0,1))\n",
    "    acc_0 = tf.reduce_mean(tf.cast(class_0_correct, 'float'))\n",
    "    \n",
    "    class_1 = tf.where(tf.equal(tf.argmax(y, 1), 1))\n",
    "    class_1 = tf.reshape(class_1, [tf.shape(class_1)[0]])\n",
    "    pred_1 = tf.gather(prediction, class_1)\n",
    "    y_1 = tf.gather(y, class_1)\n",
    "    class_1_correct = tf.equal(tf.argmax(pred_1,1), tf.argmax(y_1,1))\n",
    "    acc_1 = tf.reduce_mean(tf.cast(class_1_correct, 'float'))\n",
    "    \n",
    "    display_step = 1\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            #epoch_loss = 0\n",
    "            fold_index = 0\n",
    "            \n",
    "            kf = KFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "            for train_index, test_index in kf.split(train_features, train_labels):\n",
    "                fold_index += 1\n",
    "                X_train, X_test = train_features[train_index], train_features[test_index]\n",
    "                y_train, y_test = train_labels[train_index], train_labels[test_index]\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: X_train, y: y_train})\n",
    "                #epoch_loss += c\n",
    "                \n",
    "                train_accuracy = accuracy.eval(feed_dict={ x: X_train, y: y_train})  \n",
    "                test_accuracy = accuracy.eval(feed_dict={ x: X_test, y: y_test})  \n",
    "                \n",
    "                # increase display_step after 10 iteration of same decimal\n",
    "                if epoch%(display_step*10) == 0 and epoch:\n",
    "                       display_step *= 10\n",
    "    \n",
    "                if (epoch%display_step == 0 or (epoch+1) == hm_epochs) and fold_index == 5:\n",
    "                    print('train:%.4f, test:%.4f,  epoch %d, fold %d' % (train_accuracy, test_accuracy, epoch, fold_index))\n",
    "\n",
    "                    #if (fold_index == kf.n_splits):\n",
    "                    validation_accuracy = accuracy.eval(feed_dict={ x: validation_features, y: validation_labels})\n",
    "                    print ('val:%.2f' % (validation_accuracy))\n",
    "                    \n",
    "                    tp = true_positives.eval(feed_dict={ x: validation_features, y: validation_labels})   \n",
    "                    fp = false_positives.eval(feed_dict={ x: validation_features, y: validation_labels})  \n",
    "                    fn = false_negatives.eval(feed_dict={ x: validation_features, y: validation_labels})\n",
    "\n",
    "                    precision = float(tp) / float(tp+fn + 0.0000000000001)\n",
    "                    recall = float(tp) / float(tp + fn + 0.0000000000001)\n",
    "                    F1_val = 2 * ( precision * recall ) / ( precision + recall + 0.0000000000001 )\n",
    "\n",
    "                    x_range.append(epoch)\n",
    "                    vals.append(validation_accuracy)\n",
    "                    trains.append(train_accuracy)\n",
    "                    tests.append(test_accuracy)\n",
    "                    f1_vals.append(F1_val)\n",
    "                    \n",
    "                    #print(validation_labels)\n",
    "                    #print(class_1.eval(feed_dict={ x: validation_features, y: validation_labels})  )\n",
    "\n",
    "                    acc_1s.append(acc_1.eval(feed_dict={ x: validation_features, y: validation_labels}))\n",
    "                    acc_0s.append(acc_0.eval(feed_dict={ x: validation_features, y: validation_labels}))\n",
    "                    \n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, model_path)        \n",
    "        \n",
    "        display_stat(x_range, trains, tests, vals, acc_0s, acc_1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:0.4860, test:0.4969,  epoch 0, fold 5\n",
      "val:0.47\n",
      "train:0.4863, test:0.4988,  epoch 1, fold 5\n",
      "val:0.46\n",
      "train:0.4854, test:0.4965,  epoch 2, fold 5\n",
      "val:0.47\n",
      "train:0.4887, test:0.5035,  epoch 3, fold 5\n",
      "val:0.48\n",
      "train:0.4894, test:0.5012,  epoch 4, fold 5\n",
      "val:0.47\n",
      "train:0.4956, test:0.4942,  epoch 5, fold 5\n",
      "val:0.48\n",
      "train:0.4969, test:0.4985,  epoch 6, fold 5\n",
      "val:0.48\n",
      "train:0.4987, test:0.5074,  epoch 7, fold 5\n",
      "val:0.50\n",
      "train:0.4997, test:0.5151,  epoch 8, fold 5\n",
      "val:0.50\n",
      "train:0.4992, test:0.4950,  epoch 9, fold 5\n",
      "val:0.51\n",
      "train:0.5041, test:0.5000,  epoch 10, fold 5\n",
      "val:0.52\n",
      "train:0.5138, test:0.5236,  epoch 20, fold 5\n",
      "val:0.50\n",
      "train:0.5301, test:0.5159,  epoch 30, fold 5\n",
      "val:0.52\n",
      "train:0.5378, test:0.5399,  epoch 40, fold 5\n",
      "val:0.55\n",
      "train:0.5404, test:0.5411,  epoch 50, fold 5\n",
      "val:0.56\n",
      "train:0.5500, test:0.5589,  epoch 60, fold 5\n",
      "val:0.52\n",
      "train:0.5472, test:0.5658,  epoch 70, fold 5\n",
      "val:0.53\n",
      "train:0.5627, test:0.5515,  epoch 80, fold 5\n",
      "val:0.55\n",
      "train:0.5621, test:0.5662,  epoch 90, fold 5\n",
      "val:0.57\n",
      "train:0.5654, test:0.5778,  epoch 100, fold 5\n",
      "val:0.54\n",
      "train:0.5834, test:0.6050,  epoch 200, fold 5\n",
      "val:0.60\n",
      "train:0.5891, test:0.6053,  epoch 300, fold 5\n",
      "val:0.58\n",
      "train:0.5895, test:0.6007,  epoch 400, fold 5\n",
      "val:0.59\n",
      "train:0.5917, test:0.6154,  epoch 500, fold 5\n",
      "val:0.59\n",
      "train:0.5882, test:0.5949,  epoch 600, fold 5\n",
      "val:0.61\n",
      "train:0.5903, test:0.6030,  epoch 700, fold 5\n",
      "val:0.60\n",
      "train:0.5931, test:0.6038,  epoch 800, fold 5\n",
      "val:0.59\n",
      "train:0.5951, test:0.6061,  epoch 900, fold 5\n",
      "val:0.60\n",
      "train:0.5953, test:0.6046,  epoch 1000, fold 5\n",
      "val:0.59\n",
      "train:0.6469, test:0.6503,  epoch 1999, fold 5\n",
      "val:0.59\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAFyCAYAAAAu+3oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25dde5cd128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = tf.placeholder('float', [None, feature_count])\n",
    "y = tf.placeholder('float', [None, n_classes])\n",
    "\n",
    "model_path = \"./tmp2/model.ckpt\"\n",
    "save_dir = './tmp2/'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "    \n",
    "L2_lambda_ = 1.5e-3\n",
    "train_neural_network_CV(x, L2_lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability test [[ 0.38545015  0.61454988]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session to predict based on model\n",
    "#TODO make sure it works and test\n",
    "prediction, regularizers = neural_network_model(x)\n",
    "#Eval this to get probability of [winning,losing]\n",
    "prob = tf.nn.softmax(prediction)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     saver.restore(sess, model_path)\n",
    "    new_saver = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./tmp2'))\n",
    "    \n",
    "    #test random sample from validation test\n",
    "    prob_test = validation_features[40].reshape((1,validation_features[0].shape[0]))\n",
    "    prob_value = prob.eval(feed_dict={ x:prob_test})  \n",
    "    print('probability test', prob_value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32692872, -0.31204851, -0.32684987, -0.32688453, -0.32676454,\n",
       "        5.21943154,  0.50826235,  0.38402118, -0.31375633, -0.32673353,\n",
       "       -0.32671255, -0.32694888, -0.32693146, -0.31751632, -0.31180154,\n",
       "       -0.32656407, -0.32534424, -0.32692872, -0.31120542, -0.32684049,\n",
       "       -0.32687515, -0.32676819,  8.44878065,  0.94904214,  0.38893186,\n",
       "       -0.30494007, -0.32671912, -0.32670471, -0.32695061, -0.3269403 ,\n",
       "       -0.3168691 , -0.31103848, -0.32660903, -0.32516702, -0.32693602,\n",
       "       -0.31333459, -0.32688257, -0.32691723, -0.32682657,  3.58025366,\n",
       "        0.43102299,  0.67482447, -0.31463558, -0.32677403, -0.32673244,\n",
       "       -0.32695061, -0.32691869, -0.31811666, -0.31209724, -0.32695061,\n",
       "       -0.32602666, -0.32692689, -0.31192594, -0.32686835, -0.32690301,\n",
       "       -0.32682292,  5.78753747,  0.55221774,  0.5592773 , -0.30653076,\n",
       "       -0.3267972 , -0.32680522, -0.32692735, -0.32695061, -0.3169428 ,\n",
       "       -0.31257845, -0.32593181, -0.32630248, -0.3269351 , -0.31459225,\n",
       "       -0.32687734, -0.326912  , -0.32685019,  2.27890019,  0.26609751,\n",
       "        0.26042377, -0.31155673, -0.32688207, -0.32685833, -0.3269507 ,\n",
       "       -0.3269507 , -0.31912946, -0.31369608, -0.3269507 , -0.32537006,\n",
       "       -0.32693602, -0.31349467, -0.32688095, -0.32691561, -0.3268521 ,\n",
       "        4.10778626,  0.24002783,  0.40965818, -0.31650171, -0.32682474,\n",
       "       -0.32680249, -0.32695061, -0.32695061, -0.31767193, -0.31278076,\n",
       "       -0.32695061, -0.32695061, -0.3269351 , -0.31323972, -0.32687816,\n",
       "       -0.32691282, -0.32685539,  6.15701943,  0.5269805 ,  0.45189612,\n",
       "       -0.3139049 , -0.32673044, -0.32672022, -0.3269507 , -0.32694637,\n",
       "       -0.31819175, -0.31235719, -0.3269507 , -0.3269507 , -0.32693054,\n",
       "       -0.31278984, -0.32684381, -0.32687847, -0.32673718,  3.35458288,\n",
       "        0.55595   ,  0.40239613, -0.32191405, -0.32681197, -0.32680851,\n",
       "       -0.32694204, -0.32695061, -0.31696724, -0.31184769, -0.32506149,\n",
       "       -0.32470386, -0.32693164, -0.31240127, -0.32685428, -0.32688894,\n",
       "       -0.3267826 ,  4.66697398,  0.61270878,  0.39842881, -0.31358084,\n",
       "       -0.32678728, -0.32677411, -0.32692809, -0.32693484, -0.3163633 ,\n",
       "       -0.31168965, -0.32581041, -0.32581569, -0.32692325, -0.31527433,\n",
       "       -0.3268936 , -0.32692826, -0.32691048,  0.89425119, -0.05108737,\n",
       "        0.30523306, -0.31179533, -0.32690373, -0.32688603, -0.32694231,\n",
       "       -0.32692662, -0.32028214, -0.31461351, -0.32695061, -0.32489239])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
